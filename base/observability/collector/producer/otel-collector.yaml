apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: otel-collector
  namespace: observability
spec:
  mode: daemonset
  image: otel/opentelemetry-collector-contrib

  serviceAccount: otel-collector-producer

  hostNetwork: true
  dnsPolicy: ClusterFirstWithHostNet

  envFrom:
    - secretRef:
        name: otel-collector-config

  env:
    - name: K8S_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName

  resources:
    limits:
      cpu: 300m
      memory: 512Mi

  nodeSelector:
    kubernetes.io/os: linux

  tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule

  # host filesystem 마운트 (호스트레벨 메트릭 수집을 위해)
  volumeMounts:
    - name: hostfs
      mountPath: /hostfs
      readOnly: true

  volumes:
    - name: hostfs
      hostPath:
        path: /
        type: Directory

  #######################################################################
  # CONFIG
  #######################################################################
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

      kubeletstats:
        collection_interval: 30s
        auth_type: serviceAccount
        insecure_skip_verify: true
        metric_groups:
          - node
          - pod
          - container

      hostmetrics:
        root_path: /hostfs
        collection_interval: 30s
        scrapers:
          cpu: {}
          memory: {}
          disk: {}
          filesystem: {}
          network: {}

      prometheus/ksm:
        config:
          scrape_configs:
            - job_name: "kube-state-metrics"
              static_configs:
                - targets:
                    - "kube-state-metrics.kube-system.svc:8080"

    processors:
      memory_limiter:
        check_interval: 5s
        limit_percentage: 80
        spike_limit_percentage: 25

      batch:
        timeout: 5s
        send_batch_size: 2048

      groupbytrace:
        wait_duration: 10s
        num_traces: 10000

      tail_sampling:
        decision_wait: 10s
        num_traces: 10000
        expected_new_traces_per_sec: 100
        policies:
          - name: error-traces
            type: status_code
            status_code:
              status_codes: [ERROR]
          - name: normal-traces
            type: probabilistic
            probabilistic:
              # 모든 트레이스 수집 (FIXME: 부하 걸 때는 조정할 것!!)
              sampling_percentage: 100

      attributes/node:
        actions:
          - key: k8s.node.name
            action: upsert
            value: "${env:K8S_NODE_NAME}"

      k8sattributes:
        auth_type: serviceAccount
        filter:
          node_from_env_var: K8S_NODE_NAME
        extract:
          metadata:
            - k8s.namespace.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.container.name
            - k8s.node.name

      transform/labels:
        metric_statements:
          - context: datapoint
            statements:
              - set(attributes["k8s_pod_name"], resource.attributes["k8s.pod.name"])
              - set(attributes["k8s_namespace_name"], resource.attributes["k8s.namespace.name"])
              - set(attributes["k8s_container_name"], resource.attributes["k8s.container.name"])

      # 헬스체크 트레이스 제외
      filter/trace-healthcheck:
        traces:
          span:
            - attributes["http.route"] == "/actuator/health"

      # DB 헬스체크 트레이스 제외
      filter/trace-db-healthcheck:
        traces:
          span:
            - 'IsMatch(attributes["thread.name"], ".*housekeeper.*") == true'

    exporters:
      kafka/logs:
        brokers: "${env:KAFKA_BROKERS}"
        topic: "${env:KAFKA_LOGS_TOPIC}"
        auth:
          tls:
            insecure_skip_verify: true
          sasl:
            mechanism: "SCRAM-SHA-512"
            username: "${env:KAFKA_USERNAME}"
            password: "${env:KAFKA_PASSWORD}"

      kafka/metrics:
        brokers: "${env:KAFKA_BROKERS}"
        topic: "${env:KAFKA_METRICS_TOPIC}"
        auth:
          tls:
            insecure_skip_verify: true
          sasl:
            mechanism: "SCRAM-SHA-512"
            username: "${env:KAFKA_USERNAME}"
            password: "${env:KAFKA_PASSWORD}"

      kafka/traces:
        brokers: "${env:KAFKA_BROKERS}"
        topic: "${env:KAFKA_TRACES_TOPIC}"
        auth:
          tls:
            insecure_skip_verify: true
          sasl:
            mechanism: "SCRAM-SHA-512"
            username: "${env:KAFKA_USERNAME}"
            password: "${env:KAFKA_PASSWORD}"

      debug:
        verbosity: normal

    service:
      pipelines:
        logs:
          receivers: [otlp]
          processors: [memory_limiter, k8sattributes, batch]
          exporters: [kafka/logs]

        metrics:
          receivers: [otlp, hostmetrics, prometheus/ksm]
          processors: [memory_limiter, k8sattributes, transform/labels, batch]
          exporters: [kafka/metrics]

        metrics/kubeletstats:
          receivers: [kubeletstats]
          processors: [memory_limiter, k8sattributes, transform/labels, batch]
          exporters: [kafka/metrics, debug]

        traces:
          receivers: [otlp]
          processors: [
              memory_limiter,
              filter/trace-healthcheck,
              filter/trace-db-healthcheck,
              k8sattributes,
              # groupbytrace,
              # tail_sampling,
              batch,
            ]
          exporters: [kafka/traces]
